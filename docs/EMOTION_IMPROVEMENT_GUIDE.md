# üé≠ H∆Ø·ªöNG D·∫™N C·∫¢I THI·ªÜN EMOTION DETECTION

## üìä V·∫§N ƒê·ªÄ HI·ªÜN T·∫†I

**DeepFace** ƒëang d√πng ch·ªâ ph√°t hi·ªán 7 c·∫£m x√∫c c∆° b·∫£n:

- ‚úÖ `happy`, `sad`, `angry`, `fear`, `surprise`, `disgust`, `neutral`

**Nh∆∞ng KH√îNG ph√°t hi·ªán ƒë∆∞·ª£c:**

- ‚ùå Ch√°n n·∫£n (boredom)
- ‚ùå L·ªù ƒë·ªù (confused/dazed)
- ‚ùå M·∫•t t·∫≠p trung (distracted)

---

## üß† GI·∫¢I PH√ÅP: K·∫æT H·ª¢P MULTIPLE METRICS

### Chi·∫øn l∆∞·ª£c:

**Kh√¥ng ch·ªâ d·ª±a v√†o emotion**, m√† k·∫øt h·ª£p v·ªõi:

1. **EAR (Eye Aspect Ratio)** - ƒê·ªô m·ªü m·∫Øt
2. **Gaze Direction** - H∆∞·ªõng nh√¨n
3. **Blink Rate** - T·∫ßn su·∫•t ch·ªõp m·∫Øt
4. **Head Pose** - T∆∞ th·∫ø ƒë·∫ßu
5. **Facial Micro-expressions** - Vi bi·ªÉu c·∫£m

---

## üéØ PH√ÅT HI·ªÜN TR·∫†NG TH√ÅI PH·ª®C T·∫†P

### 1Ô∏è‚É£ **CH√ÅN N·∫¢N (Boredom)**

**ƒê·∫∑c ƒëi·ªÉm:**

- M·∫Øt m·ªü v·ª´a ph·∫£i, kh√¥ng bu·ªìn ng·ªß
- Nh√¨n l·ªách (kh√¥ng nh√¨n m√†n h√¨nh)
- ƒê·∫ßu t·ª±a ho·∫∑c ng·∫£ v·ªÅ m·ªôt b√™n
- Emotion: neutral ho·∫∑c sad
- Blink rate: b√¨nh th∆∞·ªùng ho·∫∑c th·∫•p

**C√°ch ph√°t hi·ªán:**

**V·ªã tr√≠:** `ai_models/focus_calculator.py` ho·∫∑c t·∫°o file m·ªõi `ai_models/advanced_state_detector.py`

```python
def detect_boredom(self,
                   emotion: str,
                   gaze_direction: str,
                   ear_avg: float,
                   head_pitch: float,
                   head_roll: float,
                   is_distracted: bool) -> Tuple[bool, float]:
    """
    Ph√°t hi·ªán tr·∫°ng th√°i ch√°n n·∫£n

    Returns:
        (is_bored, confidence_score)
    """
    boredom_score = 0.0

    # 1. Emotion neutral ho·∫∑c sad (30 ƒëi·ªÉm)
    if emotion in ['neutral', 'sad']:
        boredom_score += 30

    # 2. Nh√¨n l·ªách (kh√¥ng t·∫≠p trung) (25 ƒëi·ªÉm)
    if gaze_direction != 'CENTER' or is_distracted:
        boredom_score += 25

    # 3. ƒê·∫ßu nghi√™ng ho·∫∑c t·ª±a (20 ƒëi·ªÉm)
    if abs(head_roll) > 10 or abs(head_pitch) > 15:
        boredom_score += 20

    # 4. M·∫Øt m·ªü b√¨nh th∆∞·ªùng (kh√¥ng bu·ªìn ng·ªß) (15 ƒëi·ªÉm)
    if 0.20 < ear_avg < 0.30:
        boredom_score += 15

    # 5. Li√™n t·ª•c distracted (10 ƒëi·ªÉm)
    if is_distracted:
        boredom_score += 10

    is_bored = boredom_score >= 60  # Ng∆∞·ª°ng 60/100
    confidence = min(100, boredom_score)

    return is_bored, confidence
```

**Ng∆∞·ª°ng:**

- `< 40`: Kh√¥ng ch√°n
- `40-60`: C√≥ th·ªÉ ch√°n
- `> 60`: Ch·∫Øc ch·∫Øn ch√°n

---

### 2Ô∏è‚É£ **L·ªú ƒê·ªú / CONFUSED (Dazed/Confused)**

**ƒê·∫∑c ƒëi·ªÉm:**

- M·∫Øt m·ªü to (EAR cao b·∫•t th∆∞·ªùng)
- Nh√¨n th·∫≥ng nh∆∞ng "tr·ªëng r·ªóng" (kh√¥ng ch·ªõp m·∫Øt)
- ƒê·∫ßu h∆°i ng·∫£ ra sau (head_pitch √¢m)
- Emotion: surprise ho·∫∑c neutral
- Blink rate: C·ª∞C TH·∫§P (< 5 blinks/ph√∫t)

**C√°ch ph√°t hi·ªán:**

```python
def detect_dazed(self,
                 emotion: str,
                 ear_avg: float,
                 blink_count_per_minute: int,
                 head_pitch: float,
                 gaze_ratio: float) -> Tuple[bool, float]:
    """
    Ph√°t hi·ªán tr·∫°ng th√°i l·ªù ƒë·ªù/confused

    ƒê·∫∑c ƒëi·ªÉm: M·∫Øt m·ªü to, kh√¥ng ch·ªõp, nh√¨n th·∫≥ng nh∆∞ng "tr·ªëng"
    """
    dazed_score = 0.0

    # 1. M·∫Øt m·ªü to b·∫•t th∆∞·ªùng (30 ƒëi·ªÉm)
    if ear_avg > 0.30:
        dazed_score += 30

    # 2. T·∫ßn su·∫•t ch·ªõp m·∫Øt C·ª∞C TH·∫§P (25 ƒëi·ªÉm)
    if blink_count_per_minute < 8:  # B√¨nh th∆∞·ªùng: 15-20 blinks/ph√∫t
        dazed_score += 25

    # 3. Nh√¨n th·∫≥ng (gaze center) (20 ƒëi·ªÉm)
    if 0.4 < gaze_ratio < 0.6:
        dazed_score += 20

    # 4. ƒê·∫ßu h∆°i ng·∫£ ra sau (15 ƒëi·ªÉm)
    if head_pitch < -5:  # √Çm = ng·∫©ng ƒë·∫ßu
        dazed_score += 15

    # 5. Emotion surprise/neutral (10 ƒëi·ªÉm)
    if emotion in ['surprise', 'neutral']:
        dazed_score += 10

    is_dazed = dazed_score >= 55
    confidence = min(100, dazed_score)

    return is_dazed, confidence
```

**C·∫ßn th√™m:** Blink Counter (hi·ªán t·∫°i ch∆∞a c√≥)

---

### 3Ô∏è‚É£ **M·∫§T T·∫¨P TRUNG (Distracted)**

**ƒê·∫∑c ƒëi·ªÉm:**

- Nh√¨n l·ªách li√™n t·ª•c
- ƒê·∫ßu quay nhi·ªÅu
- Emotion: b·∫•t k·ª≥
- Posture x·∫•u

**C√°ch ph√°t hi·ªán:**

```python
def detect_distracted(self,
                     gaze_direction: str,
                     is_distracted_flag: bool,
                     head_yaw: float,
                     posture_score: float,
                     distraction_duration: float) -> Tuple[bool, float]:
    """
    Ph√°t hi·ªán m·∫•t t·∫≠p trung

    ƒê·∫∑c ƒëi·ªÉm: Nh√¨n l·ªách, quay ƒë·∫ßu nhi·ªÅu, t∆∞ th·∫ø x·∫•u
    """
    distraction_score = 0.0

    # 1. GazeTracker ƒë√£ ph√°t hi·ªán distracted (40 ƒëi·ªÉm)
    if is_distracted_flag:
        distraction_score += 40

    # 2. ƒê·∫ßu quay sang b√™n (30 ƒëi·ªÉm)
    if abs(head_yaw) > 20:
        distraction_score += 30

    # 3. T∆∞ th·∫ø x·∫•u (15 ƒëi·ªÉm)
    if posture_score < 50:
        distraction_score += 15

    # 4. Th·ªùi gian distracted l√¢u (15 ƒëi·ªÉm)
    if distraction_duration > 3.0:  # > 3 gi√¢y
        distraction_score += 15

    is_distracted_severe = distraction_score >= 60
    confidence = min(100, distraction_score)

    return is_distracted_severe, confidence
```

---

## üõ†Ô∏è C√ÅCH TH·ª∞C HI·ªÜN

### B∆∞·ªõc 1: T·∫°o file m·ªõi `ai_models/advanced_state_detector.py`

```python
from typing import Tuple, Dict

class AdvancedStateDetector:
    """Ph√°t hi·ªán tr·∫°ng th√°i ph·ª©c t·∫°p: ch√°n n·∫£n, l·ªù ƒë·ªù, m·∫•t t·∫≠p trung"""

    def __init__(self):
        self.blink_counter = 0
        self.blink_timestamps = []
        self.distraction_start_time = None

    def update_blink(self, is_blinking: bool):
        """C·∫≠p nh·∫≠t blink counter"""
        import time
        if is_blinking:
            self.blink_counter += 1
            self.blink_timestamps.append(time.time())

        # X√≥a blinks c≈© h∆°n 60 gi√¢y
        current_time = time.time()
        self.blink_timestamps = [t for t in self.blink_timestamps
                                 if current_time - t < 60]

    def get_blink_rate(self) -> int:
        """L·∫•y t·∫ßn su·∫•t ch·ªõp m·∫Øt (blinks/ph√∫t)"""
        return len(self.blink_timestamps)

    def detect_boredom(self, ...) -> Tuple[bool, float]:
        # Code nh∆∞ tr√™n
        pass

    def detect_dazed(self, ...) -> Tuple[bool, float]:
        # Code nh∆∞ tr√™n
        pass

    def detect_distracted(self, ...) -> Tuple[bool, float]:
        # Code nh∆∞ tr√™n
        pass

    def get_overall_state(self, ...) -> Dict:
        """Tr·∫£ v·ªÅ tr·∫°ng th√°i t·ªïng h·ª£p"""
        is_bored, bored_conf = self.detect_boredom(...)
        is_dazed, dazed_conf = self.detect_dazed(...)
        is_distracted, dist_conf = self.detect_distracted(...)

        # ∆Øu ti√™n: dazed > bored > distracted
        if is_dazed and dazed_conf > 60:
            return {'state': 'DAZED', 'confidence': dazed_conf}
        elif is_bored and bored_conf > 60:
            return {'state': 'BORED', 'confidence': bored_conf}
        elif is_distracted and dist_conf > 60:
            return {'state': 'DISTRACTED', 'confidence': dist_conf}
        else:
            return {'state': 'FOCUSED', 'confidence': 80}
```

---

### B∆∞·ªõc 2: Th√™m v√†o `main.py`

```python
from ai_models.advanced_state_detector import AdvancedStateDetector

class MainApplication:
    def __init__(self, ...):
        # ... existing code ...
        self.state_detector = AdvancedStateDetector()  # TH√äM M·ªöI
```

---

### B∆∞·ªõc 3: C·∫≠p nh·∫≠t `process_frame()` trong `main.py`

```python
def process_frame(self, ai_result: dict, frame) -> dict:
    # ... existing code ...

    # L·∫•y th√™m metrics
    head_pitch = ai_result.get('head_pitch', 0.0)  # C·∫ßn th√™m v√†o ai_processor
    head_roll = ai_result.get('head_roll', 0.0)
    head_yaw = self.gaze_tracker.current_head_yaw  # N·∫øu c√≥

    # Update blink counter
    is_blinking = ai_result.get('is_blinking', False)  # C·∫ßn th√™m v√†o drowsiness_detector
    self.state_detector.update_blink(is_blinking)
    blink_rate = self.state_detector.get_blink_rate()

    # Detect advanced states
    is_bored, bored_conf = self.state_detector.detect_boredom(
        emotion=emotion,
        gaze_direction=gaze_dir,
        ear_avg=ear_avg,
        head_pitch=head_pitch,
        head_roll=head_roll,
        is_distracted=is_distracted
    )

    is_dazed, dazed_conf = self.state_detector.detect_dazed(
        emotion=emotion,
        ear_avg=ear_avg,
        blink_count_per_minute=blink_rate,
        head_pitch=head_pitch,
        gaze_ratio=gaze_ratio
    )

    overall_state = self.state_detector.get_overall_state(...)

    return {
        **ai_result,
        # ... existing fields ...
        'is_bored': is_bored,
        'bored_confidence': bored_conf,
        'is_dazed': is_dazed,
        'dazed_confidence': dazed_conf,
        'overall_state': overall_state['state'],
        'state_confidence': overall_state['confidence'],
        'blink_rate': blink_rate
    }
```

---

### B∆∞·ªõc 4: C·∫≠p nh·∫≠t `draw_overlay()` ƒë·ªÉ hi·ªÉn th·ªã

```python
def draw_overlay(self, frame, data: dict):
    # ... existing code ...

    # Th√™m v√†o info list:
    overall_state = data.get('overall_state', 'FOCUSED')
    state_conf = data.get('state_confidence', 0)

    # Th√™m m√†u theo state
    state_colors = {
        'FOCUSED': (0, 255, 0),    # Xanh
        'DISTRACTED': (0, 255, 255), # V√†ng
        'BORED': (0, 165, 255),      # Cam
        'DAZED': (0, 0, 255)         # ƒê·ªè
    }
    state_color = state_colors.get(overall_state, (255, 255, 255))

    # Th√™m v√†o text info
    info = [
        f"State: {overall_state} ({state_conf:.0f}%)",
        # ... existing fields ...
    ]

    # V·∫Ω v·ªõi m√†u ri√™ng cho state
```

---

## üìã CHECKLIST TH·ª∞C HI·ªÜN

### Phase 1: Blink Detection (C·∫ßn thi·∫øt cho DAZED)

- [ ] S·ª≠a `drowsiness_detector.py` - th√™m blink tracking
- [ ] Tr·∫£ v·ªÅ `is_blinking` trong result

### Phase 2: Head Pose (C·∫ßn thi·∫øt cho BORED)

- [ ] S·ª≠a `posture_analyzer.py` - export head_pitch, head_roll, head_yaw
- [ ] Truy·ªÅn v√†o result t·ª´ ai_processor

### Phase 3: Advanced State Detector

- [ ] T·∫°o file `advanced_state_detector.py`
- [ ] Implement 3 methods: detect_boredom, detect_dazed, detect_distracted

### Phase 4: Integration

- [ ] Th√™m v√†o main.py
- [ ] Update process_frame()
- [ ] Update draw_overlay()

---

## üéØ K·∫æT QU·∫¢ MONG ƒê·ª¢I

| Tr·∫°ng th√°i     | ƒê·ªô ch√≠nh x√°c | Metrics ch√≠nh                    |
| -------------- | ------------ | -------------------------------- |
| **Focused**    | ~85%         | All metrics good                 |
| **Distracted** | ~80%         | Gaze l·ªách, head quay             |
| **Bored**      | ~70%         | Neutral + distracted + head tilt |
| **Dazed**      | ~65%         | M·∫Øt m·ªü to + kh√¥ng ch·ªõp           |

---

## ‚ö†Ô∏è L∆ØU √ù

1. **ƒê·ªô ch√≠nh x√°c kh√¥ng cao 100%** - c·∫ßn thu th·∫≠p data v√† fine-tune thresholds
2. **C·∫ßn calibration** - m·ªói ng∆∞·ªùi c√≥ baseline kh√°c nhau
3. **Combine v·ªõi context** - v√≠ d·ª•: th·ªùi gian h·ªçc, ƒë·ªô kh√≥ b√†i h·ªçc
4. **False positives** - c√≥ th·ªÉ ph√°t hi·ªán nh·∫ßm, c·∫ßn logic debouncing

---

**Ch√∫c b·∫°n code th√†nh c√¥ng! üé≠**
