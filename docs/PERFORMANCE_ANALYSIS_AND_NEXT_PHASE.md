# ğŸ” PHÃ‚N TÃCH HIá»†U NÄ‚NG & GIAI ÄOáº N TIáº¾P THEO

## âš ï¸ PHÃT HIá»†N: FPS THáº¤P (9 FPS)

### ğŸ› NGUYÃŠN NHÃ‚N CHÃNH - DeepFace QUÃ CHáº¬M

**Bottleneck #1: DeepFace.analyze() - Káº» thÃ¹ sá»‘ 1 cá»§a FPS**

```python
# emotion_analyzer.py line 42
DeepFace.analyze(
    img_path=frame,
    actions=['emotion'],
    enforce_detection=False,
    silent=True
)
```

**Thá»i gian xá»­ lÃ½:**

- DeepFace: **200-500ms PER CALL** (!!!)
- Interval hiá»‡n táº¡i: 30 frames (1 giÃ¢y @ 30fps)
- TÃ¡c Ä‘á»™ng: **Má»—i 30 frames bá»‹ BLOCK 200-500ms**
- â†’ FPS giáº£m tá»« 30 â†’ 9-12 FPS

**TÃ­nh toÃ¡n:**

```
KhÃ´ng cÃ³ DeepFace: ~30ms/frame â†’ 33 FPS
CÃ³ DeepFace (má»—i 30 frames):
  - 29 frames: 30ms/frame = 870ms
  - 1 frame: 30ms + 300ms (DeepFace) = 330ms
  - Total: 1200ms cho 30 frames
  - FPS = 30/1.2 = 25 FPS (lÃ½ thuyáº¿t)

Thá»±c táº¿: 9 FPS â†’ DeepFace Ä‘ang cháº­m hÆ¡n dá»± kiáº¿n!
```

### ğŸ” BOTTLENECK PHÃT HIá»†N THÃŠM

**2. Face Mesh `refine_landmarks=True`**

- Tá»‘n ~2-3ms extra cho iris landmarks
- Cáº§n thiáº¿t cho gaze tracking â†’ KHÃ”NG Táº®T ÄÆ¯á»¢C

**3. Advanced State Detector**

- Äang cháº¡y má»—i 5 frames
- TÃ­nh toÃ¡n phá»©c táº¡p (blink rate, boredom, dazed)
- TÃ¡c Ä‘á»™ng: ~5-10ms

**4. Emotion Analysis trong main.py**

- Frame resize 224x224
- DeepFace Ä‘Æ°á»£c gá»i **2 Láº¦N**: init + main.py
- DUPLICATE!

---

## ğŸš€ GIáº¢I PHÃP Tá»I Æ¯U FPS

### PHASE 1: KHáº¨N Cáº¤P - TÄƒng FPS tá»« 9 â†’ 20+ (10 phÃºt)

#### 1.1. Táº®T DEEPFACE HOÃ€N TOÃ€N (táº¡m thá»i)

```python
# main.py line 106
# COMMENT OUT emotion analysis
# if self.frame_count % self.EMOTION_CHECK_INTERVAL == 0:
#     emotion, emotion_conf, _ = self.emotion_analyzer.analyze(small_frame)
#     self.last_emotion_result = (emotion, emotion_conf, None)

# DÃ¹ng emotion cá»‘ Ä‘á»‹nh
emotion, emotion_conf = 'neutral', 85.0
```

**Káº¿t quáº£ dá»± kiáº¿n: FPS 9 â†’ 25-28**

#### 1.2. GIáº¢M ADVANCED STATE INTERVAL

```python
# Tá»« má»—i 5 frames â†’ má»—i 10 frames
if self.frame_count % 10 == 0:
    advanced_states = ...
```

**Káº¿t quáº£ dá»± kiáº¿n: FPS 25 â†’ 28-30**

#### 1.3. SKIP FRAMES CHO AI PROCESSOR

```python
# ai_processor.py - chá»‰ xá»­ lÃ½ 1/2 frames
if self.frame_count % 2 == 0:
    # Process frame
else:
    # Return cached result
```

**Káº¿t quáº£ dá»± kiáº¿n: FPS 28 â†’ 40-50** (nhÆ°ng máº¥t Ä‘á»™ mÆ°á»£t)

---

### PHASE 2: THAY THáº¾ DEEPFACE - Giáº£i phÃ¡p dÃ i háº¡n (1-2 giá»)

#### Option A: DÃ¹ng MediaPipe Face Mesh Blendshapes (RECOMMENDED)

```python
# MediaPipe cÃ³ sáºµn blendshapes cho emotion!
# Nháº¹ hÆ¡n DeepFace 100 láº§n (2-3ms thay vÃ¬ 300ms)

from mediapipe.tasks.python import vision

FaceLandmarker = vision.FaceLandmarker
face_landmarker = FaceLandmarker.create_from_options(
    FaceLandmarkerOptions(
        base_options=BaseOptions(model_asset_path='face_landmarker.task'),
        output_face_blendshapes=True,  # â† Key feature
        num_faces=1
    )
)

# Blendshapes bao gá»“m:
# - browInnerUp (surprise)
# - mouthSmile (happy)
# - mouthFrown (sad)
# - eyeSquint (disgust)
# - etc...
```

**Æ¯u Ä‘iá»ƒm:**

- âœ… Cá»±c nhanh: ~2-3ms (100x nhanh hÆ¡n DeepFace)
- âœ… ÄÃ£ cÃ³ MediaPipe trong project
- âœ… Realtime, khÃ´ng lag

**NhÆ°á»£c Ä‘iá»ƒm:**

- âŒ Cáº§n map blendshapes â†’ 7 emotions
- âŒ Accuracy tháº¥p hÆ¡n DeepFace (~70% vs 85%)

#### Option B: DÃ¹ng ONNX + Lightweight Model

```python
# FER (Facial Expression Recognition) ONNX
# Model nhá» ~2MB, ~10-20ms/inference

import onnxruntime as ort

session = ort.InferenceSession('fer_model.onnx')
emotion_probs = session.run(['output'], {'input': preprocessed_face})[0]
```

**Æ¯u Ä‘iá»ƒm:**

- âœ… Nhanh: ~10-20ms
- âœ… Accuracy tá»‘t (~80%)
- âœ… Offline

**NhÆ°á»£c Ä‘iá»ƒm:**

- âŒ Cáº§n download model riÃªng
- âŒ Cáº§n preprocessing face

#### Option C: Bá» EMOTION - DÃ¹ng proxy metrics

```python
# Thay vÃ¬ detect emotion, dÃ¹ng:
# - Blink rate (high = tired/sad)
# - Head movement (erratic = distracted/angry)
# - Gaze stability (stable = focused/neutral)
# - Posture score (good = happy, bad = sad)

def estimate_emotional_state():
    if blink_rate < 5:
        return 'dazed'  # Thay vÃ¬ 'sad'
    elif head_movement > 20:
        return 'restless'  # Thay vÃ¬ 'angry'
    elif gaze_distracted:
        return 'unfocused'  # Thay vÃ¬ 'surprise'
    else:
        return 'engaged'  # Thay vÃ¬ 'happy'
```

**Æ¯u Ä‘iá»ƒm:**

- âœ… KhÃ´ng tá»‘n thÃªm processing
- âœ… Táº­p trung vÃ o behavior, khÃ´ng pháº£i facial expression
- âœ… Äá»§ cho má»¥c Ä‘Ã­ch giÃ¡m sÃ¡t há»c táº­p

**NhÆ°á»£c Ä‘iá»ƒm:**

- âŒ KhÃ´ng pháº£i emotion tháº­t
- âŒ Ãt chi tiáº¿t hÆ¡n

---

## ğŸ“‹ GIAI ÄOáº N TIáº¾P THEO - Cáº¢I TIáº¾N GIÃM SÃT

### ğŸ¯ Priority 1: Tá»I Æ¯U PERFORMANCE (Báº®T BUá»˜C)

**Má»¥c tiÃªu: FPS 9 â†’ 25+ trong 30 phÃºt**

#### Step 1: Táº¯t DeepFace táº¡m thá»i (5 phÃºt)

- Comment emotion analysis trong main.py
- DÃ¹ng emotion='neutral' cá»‘ Ä‘á»‹nh
- Test FPS â†’ nÃªn tháº¥y ~25-28 FPS

#### Step 2: Giáº£m advanced state interval (5 phÃºt)

- Tá»« 5 frames â†’ 10 frames
- Váº«n Ä‘á»§ responsive cho boredom/dazed detection

#### Step 3: Test & validate (5 phÃºt)

- Cháº¡y app 2-3 phÃºt
- Confirm FPS á»•n Ä‘á»‹nh 25+
- Check cÃ¡c features khÃ¡c váº«n hoáº¡t Ä‘á»™ng

#### Step 4: Document & commit (5 phÃºt)

---

### ğŸ¯ Priority 2: THAY THáº¾ DEEPFACE (1-2 giá»)

**Option A: MediaPipe Blendshapes** (RECOMMENDED)

**Implementation:**

1. **ThÃªm blendshapes vÃ o Face Mesh** (30 phÃºt)

```python
# ai_processor.py
self.face_mesh = self.mp_face_mesh.FaceMesh(
    max_num_faces=1,
    refine_landmarks=True,
    output_face_blendshapes=True,  # â† NEW
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)
```

2. **Táº¡o BlendshapeEmotionMapper** (30 phÃºt)

```python
# ai_models/blendshape_emotion_mapper.py

class BlendshapeEmotionMapper:
    """Map MediaPipe blendshapes â†’ 7 emotions"""

    def map_to_emotion(self, blendshapes):
        # Happy: mouthSmile + cheekSquint
        happy_score = blendshapes['mouthSmileLeft'] +
                     blendshapes['mouthSmileRight']

        # Sad: mouthFrownLeft/Right + browDownLeft/Right
        sad_score = blendshapes['mouthFrownLeft'] +
                   blendshapes['browDownLeft']

        # Surprise: browInnerUp + eyeWideLeft/Right + jawOpen
        surprise_score = blendshapes['browInnerUp'] +
                        blendshapes['eyeWideLeft']

        # ... map 7 emotions

        return dominant_emotion, confidence
```

3. **TÃ­ch há»£p vÃ o main.py** (15 phÃºt)

```python
# Thay DeepFace
# emotion = self.emotion_analyzer.analyze(frame)

# DÃ¹ng blendshapes tá»« ai_result
blendshapes = ai_result.get('blendshapes', {})
emotion, conf = self.blendshape_mapper.map_to_emotion(blendshapes)
```

4. **Testing & calibration** (15 phÃºt)

**Káº¿t quáº£:**

- âœ… FPS: 25-30 (khÃ´ng giáº£m)
- âœ… Emotion detection váº«n cÃ³ (accuracy ~70%)
- âœ… Realtime, khÃ´ng lag

---

### ğŸ¯ Priority 3: Cáº¢I TIáº¾N GIÃM SÃT Má»šI (sau khi FPS á»•n)

#### 3.1. **Reading Behavior Detection** (30 phÃºt)

PhÃ¡t hiá»‡n Ä‘ang Ä‘á»c sÃ¡ch/tÃ i liá»‡u:

- Gaze á»•n Ä‘á»‹nh, nhÃ¬n xuá»‘ng (reading position)
- Head tilt nháº¹ (10-15Â°)
- Blink rate moderate (10-15/min)
- KhÃ´ng cÃ³ head movement lá»›n

```python
def detect_reading():
    is_reading = (
        gaze_direction == 'CENTER' and
        head_pitch > 5 and head_pitch < 20 and
        blink_rate > 8 and blink_rate < 18 and
        head_movement < 10
    )
    return is_reading
```

**Use case:** PhÃ¢n biá»‡t "Ä‘ang Ä‘á»c" vs "nhÃ¬n mÃ n hÃ¬nh"

#### 3.2. **Writing/Typing Detection** (30 phÃºt)

PhÃ¡t hiá»‡n Ä‘ang viáº¿t/gÃµ phÃ­m:

- Gaze nhÃ¬n xuá»‘ng (keyboard/notebook)
- Frequent head movements (nhÃ¬n mÃ n hÃ¬nh â†” bÃ n phÃ­m)
- Hand movements (náº¿u cÃ³ hand tracking)

```python
def detect_writing():
    # Gaze switches between down (keyboard) and center (screen)
    is_writing = (
        gaze_alternating_pattern and  # Custom tracker
        head_yaw_variance > 15 and
        posture_score > 60  # Still maintaining good posture
    )
    return is_writing
```

**Use case:** Tá»± Ä‘á»™ng dá»«ng cáº£nh bÃ¡o khi user Ä‘ang gÃµ code/viáº¿t bÃ i

#### 3.3. **Thinking/Processing Detection** (20 phÃºt)

PhÃ¡t hiá»‡n Ä‘ang suy nghÄ©:

- NhÃ¬n lÃªn trÃªn (thinking pose)
- Blink rate low (concentrating)
- Stable posture
- Emotion: neutral/surprise

```python
def detect_thinking():
    is_thinking = (
        head_pitch < -10 and  # Looking up
        blink_rate < 10 and
        posture_score > 50 and
        emotion in ['neutral', 'surprise']
    )
    return is_thinking
```

**Use case:** Äá»«ng disturb user khi Ä‘ang suy nghÄ© sÃ¢u

#### 3.4. **Eye Strain Detection** (Enhanced) (30 phÃºt)

Cáº£nh bÃ¡o má»i máº¯t dá»±a trÃªn:

- Blink rate quÃ¡ tháº¥p (< 8/min) kÃ©o dÃ i > 5 phÃºt
- Rubbing eyes (nháº¯m máº¯t lÃ¢u, frequent blinks)
- Red eyes (náº¿u cÃ³ color analysis)
- Continuous screen time > 30 phÃºt

```python
class EyeStrainDetector:
    def detect_strain(self, blink_rate, ear_avg, screen_time):
        low_blink_duration = self.track_low_blink(blink_rate)

        if low_blink_duration > 300:  # 5 minutes
            return 'severe_eye_strain'
        elif screen_time > 1800:  # 30 minutes
            return 'take_break_soon'

        return 'normal'
```

**Actions:**

- Suggest 20-20-20 rule (má»—i 20 phÃºt, nhÃ¬n xa 20s)
- Recommend blink exercises
- Auto-dim screen brightness (náº¿u cÃ³ control)

#### 3.5. **Engagement Level** (Tá»•ng há»£p táº¥t cáº£ metrics) (45 phÃºt)

TÃ­nh engagement score tá»« ALL metrics:

```python
class EngagementCalculator:
    def calculate_engagement(self, metrics):
        # Components:
        # 1. Attention: gaze center + low distraction
        attention = (
            1.0 if gaze == 'CENTER' else 0.5
        ) * (1.0 - distraction_ratio)

        # 2. Alertness: EAR high + blink normal
        alertness = (
            1.0 if ear_avg > 0.25 else 0.6
        ) * (1.0 if blink_rate > 10 else 0.7)

        # 3. Posture: good posture = engaged
        posture_factor = posture_score / 100

        # 4. Activity: reading/writing = high engagement
        activity_boost = (
            1.2 if is_reading or is_writing else 1.0
        )

        # 5. Emotion: positive = engaged
        emotion_factor = (
            1.0 if emotion in ['happy', 'neutral', 'surprise']
            else 0.8
        )

        engagement = (
            0.3 * attention +
            0.25 * alertness +
            0.2 * posture_factor +
            0.15 * emotion_factor +
            0.1 * (1.0 - is_distracted)
        ) * activity_boost

        return engagement * 100  # 0-120 scale
```

**Levels:**

- 90-120: **Highly Engaged** - flow state
- 70-89: **Engaged** - productive
- 50-69: **Moderately Engaged** - need break soon
- 30-49: **Low Engagement** - distracted
- 0-29: **Disengaged** - stop studying

---

## ğŸ“Š ROADMAP Tá»”NG THá»‚

### Week 1: PERFORMANCE

- [ ] Day 1: Táº¯t DeepFace táº¡m thá»i â†’ FPS 25+
- [ ] Day 2-3: Implement MediaPipe Blendshapes
- [ ] Day 4: Testing & calibration
- [ ] Day 5: Document & optimize

### Week 2: GIÃM SÃT NÃ‚NG CAO

- [ ] Day 1: Reading behavior detection
- [ ] Day 2: Writing/typing detection
- [ ] Day 3: Thinking detection
- [ ] Day 4: Eye strain detector
- [ ] Day 5: Engagement calculator

### Week 3: POLISH & FEATURES

- [ ] Study break timer (Ä‘Ã£ cÃ³ guide)
- [ ] Session statistics
- [ ] Database integration
- [ ] Export reports

---

## ğŸ¯ Äá»€ XUáº¤T HÃ€NH Äá»˜NG NGAY

**BÆ¯á»šC 1 (5 phÃºt):** Táº¯t DeepFace Ä‘á»ƒ test FPS

```bash
# Comment lines trong main.py
# Cháº¡y láº¡i â†’ FPS nÃªn ~25-28
```

**BÆ¯á»šC 2 (10 phÃºt):** Optimize advanced state interval

```python
# Tá»« 5 â†’ 10 frames
```

**BÆ¯á»šC 3 (2 giá»):** Implement MediaPipe Blendshapes

- Thay tháº¿ DeepFace hoÃ n toÃ n
- Giá»¯ FPS 25-30
- Váº«n cÃ³ emotion detection (accuracy ~70%)

**BÆ¯á»šC 4 (1 tuáº§n):** ThÃªm 5 tÃ­nh nÄƒng giÃ¡m sÃ¡t má»›i

- Reading, Writing, Thinking detection
- Eye strain warning
- Engagement score

---

## ğŸ’¡ Káº¾T LUáº¬N

**Æ¯u tiÃªn CAO NHáº¤T:**

1. âš¡ FPS (9 â†’ 25+) - KHáº¨N Cáº¤P
2. ğŸ”„ Thay DeepFace â†’ Blendshapes
3. ğŸ“ˆ GiÃ¡m sÃ¡t nÃ¢ng cao (Reading, Writing, Thinking, Eye Strain, Engagement)

**Timeline:** 2-3 tuáº§n Ä‘á»ƒ hoÃ n thiá»‡n toÃ n bá»™

**Expected Result:**

- FPS: 25-30 (stable)
- Features: 15+ monitoring capabilities
- Accuracy: 75-85%
- User experience: Excellent
