# üìò H∆Ø·ªöNG D·∫™N THAY TH·∫æ DEEPFACE B·∫∞NG MEDIAPIPE BLENDSHAPES

## üéØ M·ª•c ti√™u

Thay th·∫ø DeepFace (ch·∫≠m 200-500ms) b·∫±ng MediaPipe Blendshapes (nhanh 2-3ms) ƒë·ªÉ tƒÉng FPS t·ª´ 9 l√™n 25-30 m√† v·∫´n gi·ªØ ƒë∆∞·ª£c emotion detection.

---

## üìö KI·∫æN TH·ª®C N·ªÄN T·∫¢NG

### MediaPipe Blendshapes l√† g√¨?

**Blendshapes** (hay Face Blend Shapes) l√† c√°c h·ªá s·ªë m√¥ t·∫£ bi·∫øn d·∫°ng khu√¥n m·∫∑t:

- M·ªói blendshape l√† 1 s·ªë t·ª´ 0.0 ‚Üí 1.0
- Gi√° tr·ªã cao = muscle group ƒë√≥ ƒëang co/cƒÉng
- MediaPipe c√≥ **52 blendshapes** m√¥ t·∫£ ƒë·∫ßy ƒë·ªß khu√¥n m·∫∑t

**V√≠ d·ª•:**

```
mouthSmileLeft = 0.8    ‚Üí Mi·ªáng c∆∞·ªùi b√™n tr√°i (80%)
eyeBlinkLeft = 0.05     ‚Üí M·∫Øt tr√°i g·∫ßn nh∆∞ m·ªü ho√†n to√†n
browInnerUp = 0.6       ‚Üí L√¥ng m√†y nh∆∞·ªõng l√™n (surprise/fear)
```

### 52 Blendshapes c·ªßa MediaPipe

**Nh√≥m M·∫ÆT (Eyes):**

- `eyeBlinkLeft`, `eyeBlinkRight` - Nh·∫Øm m·∫Øt
- `eyeSquintLeft`, `eyeSquintRight` - Nheo m·∫Øt
- `eyeWideLeft`, `eyeWideRight` - M·ªü to m·∫Øt
- `eyeLookDownLeft/Right`, `eyeLookUpLeft/Right` - H∆∞·ªõng nh√¨n

**Nh√≥m L√îNG M√ÄY (Brows):**

- `browDownLeft`, `browDownRight` - Cau m√†y (angry/sad)
- `browInnerUp` - Nh∆∞·ªõng l√¥ng m√†y (surprise/fear)
- `browOuterUpLeft`, `browOuterUpRight` - N√¢ng l√¥ng m√†y ngo√†i

**Nh√≥m M·ªíM (Mouth):**

- `mouthSmileLeft`, `mouthSmileRight` - C∆∞·ªùi
- `mouthFrownLeft`, `mouthFrownRight` - NhƒÉn m·∫∑t (sad)
- `mouthPucker` - Chu m√¥i
- `mouthFunnel` - Tr√≤n mi·ªáng (surprise)
- `jawOpen` - H√° mi·ªáng
- `mouthUpperUpLeft/Right` - N√¢ng m√¥i tr√™n (disgust)

**Nh√≥m M√Å (Cheeks):**

- `cheekSquintLeft`, `cheekSquintRight` - M√° nhƒÉn l√™n (smile)
- `cheekPuff` - Ph·ªìng m√°

### Map Blendshapes ‚Üí 7 Emotions

#### 1. **HAPPY** üòä

**ƒê·∫∑c ƒëi·ªÉm:**

- Mi·ªáng c∆∞·ªùi (`mouthSmileLeft/Right` cao)
- M√° nhƒÉn l√™n (`cheekSquint` cao)
- M·∫Øt nheo nh·∫π (t·ª´ c∆∞·ªùi)

**C√¥ng th·ª©c:**

```python
happy_score = (
    (mouthSmileLeft + mouthSmileRight) / 2 * 0.5 +
    (cheekSquintLeft + cheekSquintRight) / 2 * 0.3 +
    eyeSquintLeft * 0.1 +
    eyeSquintRight * 0.1
)
# Range: 0.0 - 1.0
```

#### 2. **SAD** üò¢

**ƒê·∫∑c ƒëi·ªÉm:**

- M√¥i cong xu·ªëng (`mouthFrown` cao)
- L√¥ng m√†y cau (`browDown` cao)
- L√¥ng m√†y trong nh∆∞·ªõng l√™n (`browInnerUp`)

**C√¥ng th·ª©c:**

```python
sad_score = (
    (mouthFrownLeft + mouthFrownRight) / 2 * 0.4 +
    (browDownLeft + browDownRight) / 2 * 0.3 +
    browInnerUp * 0.3
)
```

#### 3. **SURPRISE** üòÆ

**ƒê·∫∑c ƒëi·ªÉm:**

- M·∫Øt m·ªü to (`eyeWide` cao)
- L√¥ng m√†y nh∆∞·ªõng cao (`browInnerUp` cao)
- Mi·ªáng h√° (`jawOpen`, `mouthFunnel`)

**C√¥ng th·ª©c:**

```python
surprise_score = (
    (eyeWideLeft + eyeWideRight) / 2 * 0.3 +
    browInnerUp * 0.3 +
    jawOpen * 0.2 +
    mouthFunnel * 0.2
)
```

#### 4. **FEAR** üò®

**ƒê·∫∑c ƒëi·ªÉm:**

- M·∫Øt m·ªü to + l√¥ng m√†y nh∆∞·ªõng (gi·ªëng surprise)
- M√¥i cƒÉng (`mouthStretch`)
- Kh√¥ng c∆∞·ªùi (kh√°c surprise)

**C√¥ng th·ª©c:**

```python
fear_score = (
    (eyeWideLeft + eyeWideRight) / 2 * 0.3 +
    browInnerUp * 0.4 +
    mouthStretchLeft * 0.15 +
    mouthStretchRight * 0.15
)
```

#### 5. **ANGRY** üò†

**ƒê·∫∑c ƒëi·ªÉm:**

- L√¥ng m√†y cau m·∫°nh (`browDown` r·∫•t cao)
- M·∫Øt nheo (`eyeSquint`)
- M√¥i cƒÉng ho·∫∑c c·∫Øn (`mouthPress`)

**C√¥ng th·ª©c:**

```python
angry_score = (
    (browDownLeft + browDownRight) / 2 * 0.5 +
    (eyeSquintLeft + eyeSquintRight) / 2 * 0.3 +
    mouthPressLeft * 0.1 +
    mouthPressRight * 0.1
)
```

#### 6. **DISGUST** ü§¢

**ƒê·∫∑c ƒëi·ªÉm:**

- M√¥i tr√™n n√¢ng l√™n (`mouthUpperUp`)
- M≈©i nhƒÉn (`noseSneer`)
- M√° nhƒÉn (`cheekSquint`)

**C√¥ng th·ª©c:**

```python
disgust_score = (
    (mouthUpperUpLeft + mouthUpperUpRight) / 2 * 0.4 +
    (noseSneerLeft + noseSneerRight) / 2 * 0.4 +
    (cheekSquintLeft + cheekSquintRight) / 2 * 0.2
)
```

#### 7. **NEUTRAL** üòê

**ƒê·∫∑c ƒëi·ªÉm:**

- T·∫§T C·∫¢ blendshapes ƒë·ªÅu th·∫•p (< 0.3)
- Khu√¥n m·∫∑t th∆∞ gi√£n

**C√¥ng th·ª©c:**

```python
# N·∫øu T·∫§T C·∫¢ emotions kh√°c < 0.3 ‚Üí NEUTRAL
all_emotions_low = max(happy, sad, surprise, fear, angry, disgust) < 0.3
if all_emotions_low:
    emotion = 'neutral'
```

---

## üîß IMPLEMENTATION - B∆Ø·ªöC 1: T·∫Øt DeepFace t·∫°m th·ªùi

**M·ª•c ti√™u:** TƒÉng FPS l√™n 25+ ngay ƒë·ªÉ test

### File: `main.py`

**T√¨m d√≤ng 106-111:**

```python
# === EMOTION ANALYSIS (m·ªói 30 frames - t·ªëi ∆∞u) ===
if self.frame_count % self.EMOTION_CHECK_INTERVAL == 0:
    # Resize frame nh·ªè ƒë·ªÉ emotion analysis nhanh h∆°n
    small_frame = cv2.resize(frame, (224, 224))
    emotion, emotion_conf, _ = self.emotion_analyzer.analyze(small_frame)
    self.last_emotion_result = (emotion, emotion_conf, None)
else:
    emotion, emotion_conf, _ = self.last_emotion_result
```

**Thay b·∫±ng:**

```python
# === EMOTION ANALYSIS - T·∫†M TH·ªúI T·∫ÆT DEEPFACE ===
# TODO: S·∫Ω thay b·∫±ng MediaPipe Blendshapes
emotion = 'neutral'
emotion_conf = 85.0
self.last_emotion_result = (emotion, emotion_conf, None)
```

**Test:**

```bash
python main.py
# FPS n√™n tƒÉng l√™n ~25-28 ngay!
```

---

## üîß IMPLEMENTATION - B∆Ø·ªöC 2: Th√™m Blendshapes v√†o Face Mesh

‚ö†Ô∏è **L∆ØU √ù QUAN TR·ªåNG:**

MediaPipe c√≥ **2 APIs kh√°c nhau**:

1. **`mediapipe.solutions`** (ƒëang d√πng) - KH√îNG h·ªó tr·ª£ blendshapes
2. **`mediapipe.tasks.python`** (m·ªõi) - C√ì blendshapes

‚Üí **PH·∫¢I CHUY·ªÇN SANG API M·ªöI!**

### File: `core/ai_processor.py`

**Step 2.1: Import th∆∞ vi·ªán m·ªõi**

**Thay ƒë·ªïi imports (d√≤ng 1-10):**

```python
import cv2
import threading
import time
import numpy as np
.
# TH√äM M·ªöI: MediaPipe Tasks API
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from typing import Optional, Dict

from ai_models.drowsiness_detector import DrowsinessDetector
from ai_models.posture_analyzer import PostureAnalyzer
from ai_models.focus_calculator import FocusCalculator
```

**Step 2.2: Download Face Landmarker Model**

MediaPipe Tasks c·∫ßn file model `.task`:

```bash
# Trong terminal
cd /home/phanhieu/Documents/MyProject_web/Smart-Learning-Support-System

# Download model (5.6MB)
wget -O face_landmarker.task https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task

# T·∫°o th∆∞ m·ª•c models
mkdir -p models
mv face_landmarker.task models/
```

**Step 2.3: Kh·ªüi t·∫°o Face Landmarker v·ªõi Blendshapes**

**Thay ƒë·ªïi `_init_models()` method (d√≤ng 42-66):**

```python
def _init_models(self) -> bool:
    try:
        print("üîÑ ƒêang kh·ªüi t·∫°o AI models...")

        # === MEDIAPIPE FACE LANDMARKER v·ªõi BLENDSHAPES ===
        base_options = python.BaseOptions(
            model_asset_path='models/face_landmarker.task'
        )

        options = vision.FaceLandmarkerOptions(
            base_options=base_options,
            output_face_blendshapes=True,  # ‚Üê KEY: B·∫≠t blendshapes!
            output_facial_transformation_matrixes=False,
            num_faces=1,
            min_face_detection_confidence=0.5,
            min_face_presence_confidence=0.5,
            min_tracking_confidence=0.5
        )

        self.face_landmarker = vision.FaceLandmarker.create_from_options(options)

        # === MEDIAPIPE POSE (gi·ªØ nguy√™n) ===
        # Import solutions cho Pose (v√¨ Tasks API ch∆∞a c√≥ Pose)
        import mediapipe as mp
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            model_complexity=0,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5,
            enable_segmentation=False
        )

        # === AI DETECTORS ===
        self.drowsiness_detector = DrowsinessDetector()
        self.posture_analyzer = PostureAnalyzer()
        self.focus_calculator = FocusCalculator()

        print("‚úÖ AI models kh·ªüi t·∫°o th√†nh c√¥ng (v·ªõi Blendshapes!)")
        return True
    except Exception as e:
        print(f"‚ùå L·ªói kh·ªüi t·∫°o AI models: {e}")
        import traceback
        traceback.print_exc()
        return False
```

**Step 2.4: S·ª≠a `_process_frame()` ƒë·ªÉ x·ª≠ l√Ω API m·ªõi**

**Thay ƒë·ªïi ph·∫ßn Face detection (d√≤ng 72-80):**

```python
def _process_frame(self, frame) -> Optional[Dict]:
    try:
        # Resize frame cho processing
        h, w = frame.shape[:2]
        scale = 320 / max(h, w)
        new_w, new_h = int(w * scale), int(h * scale)
        frame_small = cv2.resize(frame, (new_w, new_h))

        # Convert BGR ‚Üí RGB cho MediaPipe
        frame_rgb = cv2.cvtColor(frame_small, cv2.COLOR_BGR2RGB)

        # === FACE LANDMARKER (API m·ªõi) ===
        # Ph·∫£i convert sang MediaPipe Image format
        mp_image = python.vision.Image(
            image_format=python.vision.ImageFormat.SRGB,
            data=frame_rgb
        )

        # Detect face + blendshapes
        face_result = self.face_landmarker.detect(mp_image)

        # Extract landmarks v√† blendshapes
        face_landmarks = None
        blendshapes_dict = {}

        if face_result.face_landmarks and len(face_result.face_landmarks) > 0:
            # Landmarks (ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code c≈©)
            # MediaPipe Tasks tr·∫£ v·ªÅ list, c·∫ßn convert sang format c≈©
            face_landmarks = self._convert_landmarks(face_result.face_landmarks[0])

            # Blendshapes
            if face_result.face_blendshapes and len(face_result.face_blendshapes) > 0:
                blendshapes_list = face_result.face_blendshapes[0]
                # Convert th√†nh dict: {'mouthSmileLeft': 0.8, ...}
                blendshapes_dict = {
                    bs.category_name: bs.score
                    for bs in blendshapes_list
                }

        # === POSE DETECTION (gi·ªØ nguy√™n) ===
        pose_results = self.pose.process(frame_rgb)

        # === X·ª¨ L√ù TI·∫æP (gi·ªØ nguy√™n ph·∫ßn drowsiness, posture...) ===
        ear_left, ear_right, is_drowsy = 0.0, 0.0, False
        if face_landmarks is not None:
            ear_left, ear_right, is_drowsy = self.drowsiness_detector.process(face_landmarks)
        ear_avg = (ear_left + ear_right) / 2.0

        # Posture analysis
        head_tilt, shoulder_angle, posture_score, is_bad_posture = 0.0, 0.0, 100.0, False
        if pose_results.pose_landmarks:
            head_tilt, shoulder_angle, posture_score, is_bad_posture = \
                self.posture_analyzer.process(pose_results.pose_landmarks, face_landmarks)

        # Face distance
        face_distance_ipd = 0.15
        if face_landmarks is not None:
            face_distance_ipd = self.posture_analyzer.calculate_face_distance(face_landmarks)

        posture_details = self.posture_analyzer.get_posture_details()

        focus_score = self.focus_calculator.calculate_focus_score(
            ear_avg=ear_avg,
            posture_score=posture_score,
            emotion=self.current_emotion
        )

        return {
            'timestamp': time.time(),
            'ear_left': round(ear_left, 3),
            'ear_right': round(ear_right, 3),
            'ear_avg': round(ear_avg, 3),
            'head_tilt': round(head_tilt, 2),
            'shoulder_angle': round(shoulder_angle, 2),
            'posture_score': round(posture_score, 2),
            'face_distance_ipd': round(face_distance_ipd, 3),
            'posture_details': posture_details,
            'emotion': self.current_emotion,
            'emotion_confidence': round(self.emotion_confidence, 2),
            'focus_score': focus_score,
            'is_drowsy': is_drowsy,
            'is_bad_posture': is_bad_posture,
            'face_landmarks': face_landmarks,
            'blendshapes': blendshapes_dict,  # ‚Üê TH√äM M·ªöI
            'frame': frame
        }
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω frame: {e}")
        import traceback
        traceback.print_exc()
        return None

def _convert_landmarks(self, new_landmarks):
    """Convert MediaPipe Tasks landmarks ‚Üí format c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch"""
    # T·∫°o object gi·ªëng mp.solutions.face_mesh.FaceLandmark
    class LandmarkList:
        def __init__(self, landmarks):
            self.landmark = landmarks

    class Landmark:
        def __init__(self, x, y, z):
            self.x = x
            self.y = y
            self.z = z

    # Convert
    converted = [
        Landmark(lm.x, lm.y, lm.z)
        for lm in new_landmarks
    ]

    return LandmarkList(converted)
```

---

## üîß IMPLEMENTATION - B∆Ø·ªöC 3: T·∫°o Blendshape ‚Üí Emotion Mapper

### File: `ai_models/blendshape_emotion_mapper.py` (T·∫†O M·ªöI)

```python
"""
Blendshape Emotion Mapper
Map 52 MediaPipe blendshapes ‚Üí 7 emotions (happy, sad, surprise, fear, angry, disgust, neutral)
"""

from typing import Dict, Tuple
import math


class BlendshapeEmotionMapper:
    """Map MediaPipe face blendshapes to emotions"""

    # Emotion scores (gi·ªëng DeepFace)
    EMOTION_SCORES = {
        'happy': 100,
        'neutral': 85,
        'surprise': 75,
        'fear': 60,
        'sad': 45,
        'angry': 30,
        'disgust': 20
    }

    # Thresholds
    EMOTION_THRESHOLD = 0.25  # ƒêi·ªÉm t·ªëi thi·ªÉu ƒë·ªÉ x√°c ƒë·ªãnh emotion
    NEUTRAL_THRESHOLD = 0.20  # N·∫øu t·∫•t c·∫£ < threshold n√†y ‚Üí neutral

    def __init__(self):
        self.current_emotion = 'neutral'
        self.emotion_confidence = 85.0

    def map_to_emotion(self, blendshapes: Dict[str, float]) -> Tuple[str, float]:
        """
        Map blendshapes dict ‚Üí (dominant_emotion, confidence)

        Args:
            blendshapes: Dict v·ªõi keys l√† t√™n blendshape, values l√† scores 0-1

        Returns:
            (emotion_name, confidence_percentage)
        """
        if not blendshapes or len(blendshapes) == 0:
            return 'neutral', 85.0

        # T√≠nh score cho t·ª´ng emotion
        scores = {
            'happy': self._calculate_happy(blendshapes),
            'sad': self._calculate_sad(blendshapes),
            'surprise': self._calculate_surprise(blendshapes),
            'fear': self._calculate_fear(blendshapes),
            'angry': self._calculate_angry(blendshapes),
            'disgust': self._calculate_disgust(blendshapes)
        }

        # T√¨m emotion c√≥ score cao nh·∫•t
        max_emotion = max(scores.items(), key=lambda x: x[1])
        dominant_emotion, raw_score = max_emotion

        # N·∫øu T·∫§T C·∫¢ scores th·∫•p ‚Üí NEUTRAL
        if raw_score < self.NEUTRAL_THRESHOLD:
            return 'neutral', 85.0

        # Convert score 0-1 ‚Üí confidence 0-100
        confidence = min(100.0, raw_score * 100)

        # Threshold minimum
        if confidence < self.EMOTION_THRESHOLD * 100:
            return 'neutral', 85.0

        self.current_emotion = dominant_emotion
        self.emotion_confidence = confidence

        return dominant_emotion, confidence

    def _get_blendshape(self, blendshapes: Dict, key: str, default: float = 0.0) -> float:
        """Helper: L·∫•y blendshape value v·ªõi default"""
        return blendshapes.get(key, default)

    def _calculate_happy(self, bs: Dict) -> float:
        """
        Happy: C∆∞·ªùi
        - mouthSmile: cao (>0.5)
        - cheekSquint: cao (m√° nhƒÉn l√™n)
        - eyeSquint: nh·∫π (t·ª´ c∆∞·ªùi)
        """
        smile_left = self._get_blendshape(bs, 'mouthSmileLeft')
        smile_right = self._get_blendshape(bs, 'mouthSmileRight')
        cheek_left = self._get_blendshape(bs, 'cheekSquintLeft')
        cheek_right = self._get_blendshape(bs, 'cheekSquintRight')
        eye_left = self._get_blendshape(bs, 'eyeSquintLeft')
        eye_right = self._get_blendshape(bs, 'eyeSquintRight')

        smile_avg = (smile_left + smile_right) / 2
        cheek_avg = (cheek_left + cheek_right) / 2
        eye_avg = (eye_left + eye_right) / 2

        # Weighted combination
        score = (
            smile_avg * 0.5 +
            cheek_avg * 0.35 +
            eye_avg * 0.15
        )

        return score

    def _calculate_sad(self, bs: Dict) -> float:
        """
        Sad: Bu·ªìn
        - mouthFrown: cao
        - browDown: cao (cau m√†y)
        - browInnerUp: cao (nhƒÉn tr√°n)
        """
        frown_left = self._get_blendshape(bs, 'mouthFrownLeft')
        frown_right = self._get_blendshape(bs, 'mouthFrownRight')
        brow_down_left = self._get_blendshape(bs, 'browDownLeft')
        brow_down_right = self._get_blendshape(bs, 'browDownRight')
        brow_inner = self._get_blendshape(bs, 'browInnerUp')

        frown_avg = (frown_left + frown_right) / 2
        brow_down_avg = (brow_down_left + brow_down_right) / 2

        score = (
            frown_avg * 0.4 +
            brow_down_avg * 0.35 +
            brow_inner * 0.25
        )

        return score

    def _calculate_surprise(self, bs: Dict) -> float:
        """
        Surprise: Ng·∫°c nhi√™n
        - eyeWide: r·∫•t cao (m·∫Øt m·ªü to)
        - browInnerUp: cao (l√¥ng m√†y nh∆∞·ªõng)
        - jawOpen: cao (h√° mi·ªáng)
        - mouthFunnel: cao (mi·ªáng tr√≤n)
        """
        eye_left = self._get_blendshape(bs, 'eyeWideLeft')
        eye_right = self._get_blendshape(bs, 'eyeWideRight')
        brow_inner = self._get_blendshape(bs, 'browInnerUp')
        jaw_open = self._get_blendshape(bs, 'jawOpen')
        mouth_funnel = self._get_blendshape(bs, 'mouthFunnel')

        eye_avg = (eye_left + eye_right) / 2

        score = (
            eye_avg * 0.3 +
            brow_inner * 0.3 +
            jaw_open * 0.2 +
            mouth_funnel * 0.2
        )

        return score

    def _calculate_fear(self, bs: Dict) -> float:
        """
        Fear: S·ª£ h√£i
        - eyeWide: cao (gi·ªëng surprise)
        - browInnerUp: r·∫•t cao
        - mouthStretch: cao (m√¥i cƒÉng)
        - Kh√¥ng c∆∞·ªùi (kh√°c surprise)
        """
        eye_left = self._get_blendshape(bs, 'eyeWideLeft')
        eye_right = self._get_blendshape(bs, 'eyeWideRight')
        brow_inner = self._get_blendshape(bs, 'browInnerUp')
        stretch_left = self._get_blendshape(bs, 'mouthStretchLeft')
        stretch_right = self._get_blendshape(bs, 'mouthStretchRight')

        # Penalty n·∫øu c√≥ smile (fear kh√¥ng c∆∞·ªùi)
        smile_left = self._get_blendshape(bs, 'mouthSmileLeft')
        smile_right = self._get_blendshape(bs, 'mouthSmileRight')
        smile_penalty = (smile_left + smile_right) / 2

        eye_avg = (eye_left + eye_right) / 2
        stretch_avg = (stretch_left + stretch_right) / 2

        score = (
            eye_avg * 0.3 +
            brow_inner * 0.4 +
            stretch_avg * 0.3
        ) * (1.0 - smile_penalty * 0.5)  # Gi·∫£m score n·∫øu c√≥ smile

        return max(0.0, score)

    def _calculate_angry(self, bs: Dict) -> float:
        """
        Angry: T·ª©c gi·∫≠n
        - browDown: r·∫•t cao (cau m√†y m·∫°nh)
        - eyeSquint: cao (nheo m·∫Øt)
        - mouthPress: cao (c·∫Øn m√¥i)
        - jawForward: c√≥ th·ªÉ cao
        """
        brow_down_left = self._get_blendshape(bs, 'browDownLeft')
        brow_down_right = self._get_blendshape(bs, 'browDownRight')
        eye_left = self._get_blendshape(bs, 'eyeSquintLeft')
        eye_right = self._get_blendshape(bs, 'eyeSquintRight')
        press_left = self._get_blendshape(bs, 'mouthPressLeft')
        press_right = self._get_blendshape(bs, 'mouthPressRight')
        jaw_forward = self._get_blendshape(bs, 'jawForward')

        brow_avg = (brow_down_left + brow_down_right) / 2
        eye_avg = (eye_left + eye_right) / 2
        press_avg = (press_left + press_right) / 2

        score = (
            brow_avg * 0.45 +
            eye_avg * 0.25 +
            press_avg * 0.15 +
            jaw_forward * 0.15
        )

        return score

    def _calculate_disgust(self, bs: Dict) -> float:
        """
        Disgust: Gh√™ t·ªüm
        - mouthUpperUp: cao (nhƒÉn m≈©i, m√¥i tr√™n n√¢ng)
        - noseSneer: cao (nhƒÉn m≈©i)
        - cheekSquint: cao (kh√°c v·ªõi happy - kh√¥ng c√≥ smile)
        """
        upper_left = self._get_blendshape(bs, 'mouthUpperUpLeft')
        upper_right = self._get_blendshape(bs, 'mouthUpperUpRight')
        sneer_left = self._get_blendshape(bs, 'noseSneerLeft')
        sneer_right = self._get_blendshape(bs, 'noseSneerRight')
        cheek_left = self._get_blendshape(bs, 'cheekSquintLeft')
        cheek_right = self._get_blendshape(bs, 'cheekSquintRight')

        # Penalty n·∫øu c√≥ smile
        smile_left = self._get_blendshape(bs, 'mouthSmileLeft')
        smile_right = self._get_blendshape(bs, 'mouthSmileRight')
        smile_penalty = (smile_left + smile_right) / 2

        upper_avg = (upper_left + upper_right) / 2
        sneer_avg = (sneer_left + sneer_right) / 2
        cheek_avg = (cheek_left + cheek_right) / 2

        score = (
            upper_avg * 0.4 +
            sneer_avg * 0.4 +
            cheek_avg * 0.2
        ) * (1.0 - smile_penalty * 0.7)  # Gi·∫£m m·∫°nh n·∫øu c√≥ smile

        return max(0.0, score)

    def get_emotion_score(self, emotion: str = None) -> float:
        """L·∫•y focus score t·ª´ emotion (gi·ªëng DeepFace)"""
        if emotion is None:
            emotion = self.current_emotion
        return self.EMOTION_SCORES.get(emotion.lower(), 50.0)

    def get_current_state(self) -> Dict:
        """L·∫•y tr·∫°ng th√°i hi·ªán t·∫°i"""
        return {
            'emotion': self.current_emotion,
            'confidence': self.emotion_confidence,
            'focus_score': self.get_emotion_score()
        }
```

---

## üîß IMPLEMENTATION - B∆Ø·ªöC 4: T√≠ch h·ª£p v√†o main.py

### File: `main.py`

**Th√™m import (d√≤ng 9):**

```python
from ai_models.blendshape_emotion_mapper import BlendshapeEmotionMapper
```

**Kh·ªüi t·∫°o trong `__init__` (d√≤ng 24):**

```python
self.advanced_state_detector = AdvancedStateDetector()
self.blendshape_mapper = BlendshapeEmotionMapper()  # ‚Üê TH√äM M·ªöI
self.calibrator = Calibrator()
```

**Thay ƒë·ªïi trong `process_frame()` (d√≤ng 106-111):**

```python
# === EMOTION ANALYSIS - MediaPipe Blendshapes ===
blendshapes = ai_result.get('blendshapes', {})
if blendshapes:
    emotion, emotion_conf = self.blendshape_mapper.map_to_emotion(blendshapes)
else:
    emotion, emotion_conf = 'neutral', 85.0
```

**X√≥a ph·∫ßn DeepFace c≈© ho√†n to√†n:**

```python
# X√ìA:
# if self.frame_count % self.EMOTION_CHECK_INTERVAL == 0:
#     small_frame = cv2.resize(frame, (224, 224))
#     emotion, emotion_conf, _ = self.emotion_analyzer.analyze(small_frame)
#     self.last_emotion_result = (emotion, emotion_conf, None)
# else:
#     emotion, emotion_conf, _ = self.last_emotion_result
```

---

## üß™ TESTING & CALIBRATION

### Test 1: Ki·ªÉm tra FPS

```bash
python main.py
# Quan s√°t FPS ·ªü g√≥c ph·∫£i tr√™n
# Expected: 25-30 FPS (kh√¥ng gi·∫£m khi emotion detection ch·∫°y)
```

### Test 2: Ki·ªÉm tra Emotions

L√†m c√°c bi·ªÉu c·∫£m v√† quan s√°t:

1. **HAPPY** üòä - C∆∞·ªùi toe ‚Üí n√™n hi·ªán "happy"
2. **SAD** üò¢ - Cau m√†y, nhƒÉn tr√°n ‚Üí "sad"
3. **SURPRISE** üòÆ - M·∫Øt m·ªü to, h√° mi·ªáng ‚Üí "surprise"
4. **ANGRY** üò† - Cau m√†y m·∫°nh, nheo m·∫Øt ‚Üí "angry"
5. **DISGUST** ü§¢ - NhƒÉn m≈©i, n√¢ng m√¥i tr√™n ‚Üí "disgust"
6. **FEAR** üò® - Gi·ªëng surprise nh∆∞ng cƒÉng th·∫≥ng ‚Üí "fear"
7. **NEUTRAL** üòê - M·∫∑t th∆∞ gi√£n ‚Üí "neutral"

### Test 3: Debug Blendshapes

Th√™m print ƒë·ªÉ xem raw blendshapes:

```python
# Trong main.py process_frame()
blendshapes = ai_result.get('blendshapes', {})
if blendshapes:
    # Debug: In top 5 blendshapes cao nh·∫•t
    sorted_bs = sorted(blendshapes.items(), key=lambda x: x[1], reverse=True)[:5]
    print(f"Top blendshapes: {sorted_bs}")

    emotion, emotion_conf = self.blendshape_mapper.map_to_emotion(blendshapes)
```

### Calibration: ƒêi·ªÅu ch·ªânh Thresholds

N·∫øu emotion detection kh√¥ng ch√≠nh x√°c:

**1. Qu√° nhi·ªÅu "neutral"** ‚Üí Gi·∫£m `NEUTRAL_THRESHOLD`:

```python
# blendshape_emotion_mapper.py
NEUTRAL_THRESHOLD = 0.15  # T·ª´ 0.20 ‚Üí 0.15
```

**2. Emotion nh·∫£y li√™n t·ª•c** ‚Üí TƒÉng `EMOTION_THRESHOLD`:

```python
EMOTION_THRESHOLD = 0.30  # T·ª´ 0.25 ‚Üí 0.30
```

**3. Happy qu√° nh·∫°y** ‚Üí Gi·∫£m weight c·ªßa cheekSquint:

```python
# _calculate_happy()
score = (
    smile_avg * 0.6 +      # TƒÉng
    cheek_avg * 0.25 +     # Gi·∫£m
    eye_avg * 0.15
)
```

**4. Sad kh√¥ng nh·∫≠n** ‚Üí TƒÉng weight c·ªßa browInnerUp:

```python
# _calculate_sad()
score = (
    frown_avg * 0.35 +     # Gi·∫£m
    brow_down_avg * 0.30 + # Gi·∫£m
    brow_inner * 0.35      # TƒÉng
)
```

---

## üìä SO S√ÅNH: DeepFace vs Blendshapes

| Metric           | DeepFace           | Blendshapes        |
| ---------------- | ------------------ | ------------------ |
| **T·ªëc ƒë·ªô**       | 200-500ms          | 2-3ms              |
| **FPS Impact**   | Gi·∫£m 9-12 FPS      | Kh√¥ng gi·∫£m         |
| **Accuracy**     | 85-90%             | 65-75%             |
| **Latency**      | High (block)       | None               |
| **Offline**      | ‚úÖ                 | ‚úÖ                 |
| **Dependencies** | TensorFlow (large) | MediaPipe (c√≥ s·∫µn) |
| **Model size**   | ~100MB             | ~5MB               |
| **7 Emotions**   | ‚úÖ                 | ‚úÖ                 |
| **Realtime**     | ‚ùå                 | ‚úÖ                 |

**K·∫øt lu·∫≠n:** Blendshapes t·ªët h∆°n cho real-time monitoring!

---

## üêõ TROUBLESHOOTING

### L·ªói 1: "No module named 'mediapipe.tasks'"

```bash
# C√†i ƒë·∫∑t MediaPipe version m·ªõi nh·∫•t
pip install --upgrade mediapipe
# Ho·∫∑c
pip install mediapipe>=0.10.0
```

### L·ªói 2: "model_asset_path not found"

```bash
# Ki·ªÉm tra file model t·ªìn t·∫°i
ls -lh models/face_landmarker.task

# N·∫øu kh√¥ng c√≥, download l·∫°i:
wget -O models/face_landmarker.task \
  https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task
```

### L·ªói 3: "AttributeError: 'NoneType' has no attribute 'face_landmarks'"

```python
# Trong _process_frame(), th√™m check:
if face_result.face_landmarks is None or len(face_result.face_landmarks) == 0:
    face_landmarks = None
    blendshapes_dict = {}
```

### L·ªói 4: FPS v·∫´n th·∫•p

```python
# Gi·∫£m resolution processing xu·ªëng 240p:
scale = 240 / max(h, w)  # T·ª´ 320 ‚Üí 240

# Ho·∫∑c skip frames:
if self.frame_count % 2 == 0:
    # Process
else:
    # Return cached
```

### L·ªói 5: Emotion detection kh√¥ng ch√≠nh x√°c

```python
# TƒÉng smoothing - l∆∞u history 5 frames:
class BlendshapeEmotionMapper:
    def __init__(self):
        self.emotion_history = []

    def map_to_emotion(self, blendshapes):
        emotion, conf = self._calculate(blendshapes)

        # Smoothing
        self.emotion_history.append(emotion)
        if len(self.emotion_history) > 5:
            self.emotion_history.pop(0)

        # Most common emotion
        from collections import Counter
        dominant = Counter(self.emotion_history).most_common(1)[0][0]

        return dominant, conf
```

---

## ‚úÖ CHECKLIST HO√ÄN TH√ÄNH

- [ ] **B∆∞·ªõc 1:** Comment DeepFace, test FPS (~25)
- [ ] **B∆∞·ªõc 2.1:** Import MediaPipe Tasks API
- [ ] **B∆∞·ªõc 2.2:** Download face_landmarker.task
- [ ] **B∆∞·ªõc 2.3:** Init Face Landmarker v·ªõi blendshapes
- [ ] **B∆∞·ªõc 2.4:** S·ª≠a \_process_frame() x·ª≠ l√Ω API m·ªõi
- [ ] **B∆∞·ªõc 3:** T·∫°o blendshape_emotion_mapper.py
- [ ] **B∆∞·ªõc 4:** T√≠ch h·ª£p v√†o main.py
- [ ] **Test:** FPS 25-30 ·ªïn ƒë·ªãnh
- [ ] **Test:** Emotions detection ho·∫°t ƒë·ªông
- [ ] **Calibration:** ƒêi·ªÅu ch·ªânh thresholds n·∫øu c·∫ßn

---

## üéØ K·∫æT QU·∫¢ MONG ƒê·ª¢I

**TR∆Ø·ªöC (DeepFace):**

- FPS: 9-12
- Emotion accuracy: 85%
- Latency: 200-500ms
- Experience: Lag, kh√¥ng realtime

**SAU (Blendshapes):**

- FPS: 25-30 ‚úÖ
- Emotion accuracy: 70-75%
- Latency: 2-3ms ‚úÖ
- Experience: Smooth, realtime ‚úÖ

**Trade-off ch·∫•p nh·∫≠n ƒë∆∞·ª£c:**

- Gi·∫£m accuracy 10-15% ƒë·ªÉ c√≥ FPS cao g·∫•p 3 l·∫ßn
- Cho m·ª•c ƒë√≠ch gi√°m s√°t h·ªçc t·∫≠p: ƒê·ª¶ T·ªêT!

---

## üìö T√ÄI LI·ªÜU THAM KH·∫¢O

**MediaPipe Documentation:**

- Face Landmarker: https://developers.google.com/mediapipe/solutions/vision/face_landmarker
- Blendshapes list: https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png
- Python API: https://developers.google.com/mediapipe/api/solutions/python/mp/tasks/vision

**Model Download:**

- Face Landmarker: https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task

**Tham kh·∫£o th√™m:**

- ARKit Blendshapes: https://developer.apple.com/documentation/arkit/arfaceanchor/blendshapelocation
- Facial Action Coding System (FACS): https://en.wikipedia.org/wiki/Facial_Action_Coding_System
