from core.camera_thread import CameraThread
from core.ai_processor import AIProcessorThread
from ai_models.gaze_tracker import GazeTracker
from ai_models.emotion_analyzer import EmotionAnalyzer
# from ai_models.phone_detector import PhoneDetector  # ƒê√£ t·∫Øt ƒë·ªÉ tƒÉng FPS
from ai_models.focus_calculator import FocusCalculator
from ai_models.calibrator import Calibrator
from ai_models.adaptive_detector import AdaptiveDetector
from ai_models.advanced_state_detector import AdvancedStateDetector
from database.db_manager import DatabaseManager
import cv2 
import time
from queue import Queue, Empty

class MainApplication:
    def __init__(self, camera_index: int = 0):
        self.frame_queue = Queue(maxsize=2)
        self.result_queue = Queue(maxsize=2)
        self.camera_thread = CameraThread(camera_index, self.frame_queue)
        self.ai_thread = AIProcessorThread(self.frame_queue, self.result_queue)
        self.gaze_tracker = GazeTracker()
        self.emotion_analyzer = EmotionAnalyzer(analysis_interval=45)  # T·ªëi ∆∞u: 45 frames
        self.focus_calculator = FocusCalculator()
        self.advanced_state_detector = AdvancedStateDetector()  # Ph√°t hi·ªán: boredom, dazed, severe distraction
        self.calibrator = Calibrator()
        self.db_manager = DatabaseManager()
        self.running = False
        self.is_calibrated = False
        self.current_focus_score = 0.0
        
        # Frame counter ƒë·ªÉ skip heavy operations
        self.frame_count = 0
        # Phone detector ƒê√É T·∫ÆT
        # self.PHONE_CHECK_INTERVAL = 5
        self.EMOTION_CHECK_INTERVAL = 30  # Emotion m·ªói 30 frames (1 gi√¢y)
        # self.last_phone_result = (False, 0.0, [])
        self.last_emotion_result = ('neutral', 0.0, None)
        
        # FPS tracking
        self.fps_start_time = time.time()
        self.fps_frame_count = 0
        self.current_fps = 0.0
    def start(self):
        print("Starting Main Application...")
        self.camera_thread.start()
        self.ai_thread.start()
        self.running = True
    def stop(self):
        print("Stopping Main Application...")
        self.running = False
        self.camera_thread.stop()
        self.ai_thread.stop()
        cv2.destroyAllWindows()
    def run(self):
        self.start()
        while self.running:
            try:
                result = self.result_queue.get(timeout=0.1)
                frame = result['frame']
                processed = self.process_frame(result, frame)
                display_frame = self.draw_overlay(frame, processed)
                cv2.imshow("Smart Learning Support System", display_frame)
            except Empty:
                pass
            
            # X·ª≠ l√Ω ph√≠m b·∫•m (lu√¥n ch·∫°y)
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('c'):
                self.calibrate()
        
        self.stop()

    def calibrate(self):
        """Ch·∫°y calibration 10 gi√¢y"""
        print("üîÑ B·∫Øt ƒë·∫ßu calibration - Gi·ªØ t∆∞ th·∫ø b√¨nh th∆∞·ªùng...")
        self.calibrator.start()
        # TODO: Implement full calibration logic
        print("‚úÖ Calibration placeholder - c·∫ßn implement ƒë·∫ßy ƒë·ªß")

    def process_frame(self, ai_result: dict, frame) -> dict:
        """X·ª≠ l√Ω frame v·ªõi t·∫•t c·∫£ AI models - T·ªêI ∆ØU PERFORMANCE"""
        self.frame_count += 1
        
        # === FPS CALCULATION ===
        self.fps_frame_count += 1
        elapsed = time.time() - self.fps_start_time
        if elapsed >= 1.0:
            self.current_fps = self.fps_frame_count / elapsed
            self.fps_frame_count = 0
            self.fps_start_time = time.time()
        
        # L·∫•y d·ªØ li·ªáu t·ª´ AI Processor
        ear_avg = ai_result.get('ear_avg', 0.25)
        posture_score = ai_result.get('posture_score', 100.0)
        face_landmarks = ai_result.get('face_landmarks', None)
        
        # === GAZE TRACKING (nh·∫π - ch·∫°y m·ªói frame) ===
        if face_landmarks is not None:
            gaze_ratio, gaze_dir, is_distracted = self.gaze_tracker.process(face_landmarks)
        else:
            gaze_ratio, gaze_dir, is_distracted = 0.5, "CENTER", False
        
        # === EMOTION ANALYSIS (m·ªói 30 frames - t·ªëi ∆∞u) ===
        if self.frame_count % self.EMOTION_CHECK_INTERVAL == 0:
            # Resize frame nh·ªè ƒë·ªÉ emotion analysis nhanh h∆°n
            small_frame = cv2.resize(frame, (224, 224))  # Nh·ªè h∆°n = nhanh h∆°n
            emotion, emotion_conf, _ = self.emotion_analyzer.analyze(small_frame)
            self.last_emotion_result = (emotion, emotion_conf, None)
        else:
            emotion, emotion_conf, _ = self.last_emotion_result
        # === FACE DISTANCE MONITORING ===
        # IPD c√†ng L·ªöN ‚Üí c√†ng G·∫¶N camera, IPD c√†ng NH·ªé ‚Üí c√†ng XA camera
        face_distance_ipd = ai_result.get('face_distance_ipd', 0.15)
        
        if face_distance_ipd > 0.2:  # IPD L·ªöN = G·∫¶N
            distance_status = "too_close"  # FIX: ƒë·ªïi t·ª´ "Too Far" ‚Üí "too_close"
            is_too_close = True
            is_too_far = False
        elif face_distance_ipd < 0.1:  # IPD NH·ªé = XA
            distance_status = "too_far"  # FIX: ƒë·ªïi t·ª´ "Too Close" ‚Üí "too_far"
            is_too_close = False
            is_too_far = True
        else:
            distance_status = "good"
            is_too_close = False
            is_too_far = False
        
        # ∆Ø·ªõc t√≠nh kho·∫£ng c√°ch: IPD 0.2 ‚âà 35cm, 0.15 ‚âà 50cm, 0.1 ‚âà 75cm
        estimated_distance_cm = int(50 / (face_distance_ipd / 0.15)) if face_distance_ipd > 0 else 50
        
        
        # === ADVANCED STATE DETECTION (Boredom, Dazed, Severe Distraction) ===
        # L·∫•y head angles t·ª´ posture analyzer (c·∫ßn cho c·∫£ advanced state v√† microsleep)
        posture_details = ai_result.get('posture_details', {})
        head_pitch = posture_details.get('head_pitch', 0.0)
        head_roll = posture_details.get('head_roll', 0.0)
        head_yaw = posture_details.get('head_yaw', 0.0)
        
        # T·ªëi ∆∞u: Ch·ªâ ch·∫°y advanced state detection m·ªói 5 frames ƒë·ªÉ tƒÉng FPS
        if self.frame_count % 5 == 0:
            advanced_states = self.advanced_state_detector.process_all_states(
                ear_avg=ear_avg,
                emotion=emotion,
                emotion_conf=emotion_conf,
                head_pitch=head_pitch,
                head_roll=head_roll,
                head_yaw=head_yaw,
                gaze_direction=gaze_dir,
                is_using_phone=False,  # Phone detector ƒë√£ t·∫Øt
                posture_score=posture_score
            )
            # L∆∞u k·∫øt qu·∫£ ƒë·ªÉ d√πng cho c√°c frame kh√°c
            self.last_advanced_states = advanced_states
        else:
            # D√πng k·∫øt qu·∫£ c≈©
            advanced_states = getattr(self, 'last_advanced_states', {
                'is_bored': False,
                'is_dazed': False,
                'is_severely_distracted': False,
                'blink_rate': 0.0,
                'dominant_state': 'normal',
                'warning_message': ''
            })
        
        # Micro-sleep detection (ch·∫°y m·ªói frame v√¨ quan tr·ªçng)
        is_microsleep, micro_duration = self.ai_thread.drowsiness_detector.detect_microsleep(
            ear_avg=ear_avg,
            head_pitch=head_pitch,
            head_yaw=head_yaw,
            head_roll=head_roll
        ) 
        
        # === FOCUS SCORE (ch·ªâ t·∫≠p trung v√†o: drowsiness, posture, gaze) ===
        focus_score = self.focus_calculator.calculate_focus_score(
            ear_avg=ear_avg,
            posture_score=posture_score,
            emotion=emotion,
            gaze_ratio=gaze_ratio,
            is_distracted=is_distracted,
            is_using_phone=False  # Phone detector ƒë√£ t·∫Øt
        )
        
        return {
            **ai_result,
            'gaze_ratio': round(gaze_ratio, 3),
            'gaze_direction': gaze_dir,
            'is_distracted': is_distracted,
            'emotion': emotion,
            'emotion_confidence': round(emotion_conf, 1),
            'focus_score': focus_score,
            'focus_level': self.focus_calculator.get_focus_level(),
            # Advanced states
            'advanced_states': advanced_states,
            'is_bored': advanced_states['is_bored'],
            'is_dazed': advanced_states['is_dazed'],
            'is_severely_distracted': advanced_states['is_severely_distracted'],
            'blink_rate': advanced_states['blink_rate'],
            'face_distance_ipd': face_distance_ipd,
            'distance_status': distance_status,
            'estimated_distance_cm': estimated_distance_cm,
            'is_too_close': is_too_close,
            'is_too_far': is_too_far,
            'is_microsleep': is_microsleep,
            'microsleep_duration': micro_duration
        }
    def draw_overlay(self, frame, data: dict):
        """V·∫Ω th√¥ng tin l√™n frame"""
        h, w = frame.shape[:2]
        
        # Background cho text (l√†m r·ªông th√™m cho advanced states)
        cv2.rectangle(frame, (10, 10), (420, 270), (0 , 0, 0), -1)
        cv2.rectangle(frame, (10, 10), (420, 270), (255, 255, 255), 2)        
        focus_level = data.get('focus_level', {})
        emoji = focus_level.get('emoji', '')
        
        # M√†u theo focus level
        focus_score = data.get('focus_score', 0)
        if focus_score >= 75:
            color = (0, 255, 0)  # Xanh l√°
        elif focus_score >= 50:
            color = (0, 255, 255)  # V√†ng
        else:
            color = (0, 0, 255)  # ƒê·ªè
        
        # Text th√¥ng tin - T·∫¨P TRUNG, BU·ªíN NG·ª¶, T∆Ø TH·∫æ
        y = 35
        
        # Advanced states
        advanced_states = data.get('advanced_states', {})
        dominant_state = advanced_states.get('dominant_state', 'normal')
        blink_rate = advanced_states.get('blink_rate', 0.0)
        distance_cm = data.get('estimated_distance_cm', 0)
        distance_status = data.get('distance_status', 'unknown')
        if distance_status == 'too_close':
            distance_color = (0, 0, 255)
        elif distance_status == 'too_far':
            distance_color = (255 , 165, 0)
        else:
            distance_color = (0, 255, 0)
        info = [
            f"Focus: {focus_score:.1f} {emoji}",
            f"Drowsy: {'YES!' if data.get('is_drowsy') else 'NO'} (EAR: {data.get('ear_avg', 0):.3f})",
            f"Posture: {data.get('posture_score', 0):.1f} {'(BAD!)' if data.get('is_bad_posture') else '(Good)'}",
            f"Gaze: {data.get('gaze_direction', 'CENTER')} {'(Distracted!)' if data.get('is_distracted') else ''}",
            f"Emotion: {data.get('emotion', 'neutral')} ({data.get('emotion_confidence', 0):.0f}%)",
            f"Blink Rate: {blink_rate:.1f} blinks/min",
            f"State: {dominant_state.upper()}"
        ]
        
        for i, text in enumerate(info):
            # Focus score d√πng m√†u ƒë·∫∑c bi·ªát
            text_color = color if i == 0 else (255, 255, 255)
            # Dominant state m√†u ƒë·ªè n·∫øu kh√¥ng normal
            if i == 6 and dominant_state != 'normal':
                text_color = (0, 0, 255)
            cv2.putText(frame, text, (20, y), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.55, text_color, 2)
            y += 26
        cv2.putText(frame, f"Distance: ~{distance_cm}cm",
           (20, y),
           cv2.FONT_HERSHEY_SIMPLEX, 0.55, distance_color, 2)
        # C·∫£nh b√°o ∆∞u ti√™n cao nh·∫•t: Advanced states > Drowsy > Bad posture
        advanced_states = data.get('advanced_states', {})
        warning_msg = advanced_states.get('warning_message', '')
        if data.get('is_microsleep'):
    # C·∫£nh b√°o ƒë·ªè nh·∫•p nh√°y
            if int(time.time() * 2) % 2 == 0:  # Blink effect
                cv2.rectangle(frame, (0, 0), (w, h), (0, 0, 255), 10)
                cv2.putText(frame, "!!! MICRO-SLEEP DETECTED !!!",
                        (w//2 - 200, h//2),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 4)

                duration_sec = data.get('microsleep_duration', 0) / 30
                cv2.putText(frame, f"Duration: {duration_sec:.1f}s",
                        (w//2 - 100, h//2 + 50),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        elif warning_msg:  # Bored, Dazed, ho·∫∑c Severely Distracted
            cv2.putText(frame, warning_msg, (w//2 - 200, 50), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 3)
        elif data.get('is_too_close'):  # ‚Üê TH√äM: Distance warning
            cv2.putText(frame, "TOO CLOSE TO SCREEN!", (w//2 - 180, 50),
                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 3)
        elif data.get('is_too_far'):
            cv2.putText(frame, "TOO FAR FROM CAMERA!", (w//2 - 180, 50),
                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 165, 0), 3)
        elif data.get('is_drowsy'):
            cv2.putText(frame, "DROWSY WARNING!", (w//2 - 120, 50), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)
        elif data.get('is_bad_posture'):
            cv2.putText(frame, "BAD POSTURE!", (w//2 - 100, 50), 
                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 165, 255), 3)
        
        # FPS display
        cv2.putText(frame, f"FPS: {self.current_fps:.1f}", 
                    (w - 100, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
        
        # Ph√≠m t·∫Øt
        cv2.putText(frame, "Press 'q' to quit, 'c' to calibrate", 
                    (10, h - 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
        
        return frame
if __name__ == "__main__":
    app = MainApplication(camera_index=0)
    try:
        app.run()
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Interrupted by user")
    finally:
        app.stop()